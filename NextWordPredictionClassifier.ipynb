{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad942f7b-279e-4057-b090-754ae8dca1df",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c06aed-6182-4347-920b-ba75071fdd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "import pandas as pd\n",
    "\n",
    "def create_dataset(path = \"/Users/arunkaul/Desktop/Random.txt\"): #Add The Path Of Text File That Contains All The Text On Which You will Train Your Model\n",
    "    n = 5\n",
    "    dataset_book = open(path,\"r\")\n",
    "    \n",
    "    list_of_lines = dataset_book.readlines()\n",
    "    list_of_lines_data = []\n",
    "    list_data = []\n",
    "    # Cleaning Data\n",
    "    for line in list_of_lines:\n",
    "        list_of_lines_data.append(line.replace(\"?\",\"\").replace(\",\",\"\").replace(\".\",\"\").replace(\"!\",\"\").replace(\"\\n\",\"\").replace(\"<s>\",\"\").replace(\"</s>\",\"\").replace(\"'\",\"\").strip())\n",
    "    \n",
    "    list_of_lines_data = list_of_lines_data[1:len(list_of_lines_data)-1]\n",
    "\n",
    "    #Adding The Text Data To New List\n",
    "    for i in list_of_lines_data:\n",
    "        if len(i) <= 6:\n",
    "            list_of_lines_data.remove(i)\t\n",
    "\n",
    "    print(list_of_lines_data)\n",
    "    for i in list_of_lines_data:\n",
    "        i = i.split(\" \")\n",
    "        for a in i:\n",
    "            list_data.append(a)\n",
    "    \n",
    "    lines = []\n",
    "    word = []\n",
    "    lines_data = []\n",
    "    index = []\n",
    "\n",
    "    #Converting that List Into Data Of 2 word Lines As Input And 1 Word As Output\n",
    "    \n",
    "    for i in range(0,len(list_data)-n,n):\n",
    "        lines.append(list_data[i:i+n])\n",
    "        word.append(list_data[i+n])\n",
    "        \n",
    "    c = 0\n",
    "    for i in lines:\n",
    "        c+=1\n",
    "        lines_data.append(i[0]+\" \"+ i[1]+\" \"+i[2]+\" \"+i[3]+\" \" + i[4])\n",
    "        index.append(c)  \n",
    "        \n",
    "    # Creating A Pandas Dataframe For Further Use\n",
    "    dataset = {'lines':lines_data,'word':word}\n",
    "    dataset_final = pd.DataFrame(dataset)\n",
    "    dataset_final[\"lines\"] = dataset_final[\"lines\"].apply(lambda x: str(x).lower())\n",
    "    dataset_final.to_csv(\"File.csv\")\n",
    "    return dataset_final\n",
    "\n",
    "dataset_books = create_dataset()\n",
    "dataset_books.tail(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a1708-2d54-4ae9-a99f-8438c8f740dd",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdad7d8-dca2-4b1f-8cda-9f231c1cf5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow\n",
    "\n",
    "features = 200000\n",
    "\n",
    "\n",
    "tokenizer_y = Tokenizer(features)\n",
    "\n",
    "#Fitting The Values To Words\n",
    "tokenizer_y.fit_on_texts(dataset_books['word'].values)\n",
    "\n",
    "#Converting Words To NumPy Array\n",
    "Y = tokenizer_y.texts_to_sequences(dataset_books['word'].values)\n",
    "\n",
    "#Converting Lines To NumPy Array\n",
    "X = tokenizer_y.texts_to_sequences(dataset_books['lines'].values)\n",
    "\n",
    "# Removing Wrong Data\n",
    "c = 0\n",
    "D = 0\n",
    "for i in X:\n",
    "    if(len(i) > 5):\n",
    "        c +=1\n",
    "        index = X.index(i)\n",
    "        Y.pop(index)\n",
    "        X.remove(i)\n",
    "\n",
    "for i in Y:\n",
    "    if(len(i) > 1):\n",
    "        D +=1\n",
    "        index = Y.index(i)\n",
    "        X.pop(index)\n",
    "        Y.remove(i)\n",
    "        \n",
    "X = pad_sequences(X,maxlen = 5)\n",
    "Y = pad_sequences(Y,maxlen = 1)\n",
    "\n",
    "#Creating Traing Data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.15, random_state=42)\n",
    "print(Y)\n",
    "print(tokenizer_y.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3c39af-7236-4a30-a24c-1eca05162db7",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d4aeaf-572e-4ec8-8ee6-dfcf6b722e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import *\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(features,32))\n",
    "model.add(GRU(600, return_sequences=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(11803,activation = \"softmax\"))\n",
    "          \n",
    "print(X_train.shape)\n",
    "model.build(input_shape = X_train.shape)\n",
    "\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_accuracy',\n",
    "                                              patience=2)\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "info = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=30,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    callbacks = [callback]\n",
    ")\n",
    "\n",
    "model.save('model_6.keras')\n",
    "plt.title(\"Loss Analysis\")\n",
    "plt.plot(info.history[\"loss\"], label=\"Loss\")\n",
    "plt.plot(info.history[\"accuracy\"], label=\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb89976-1206-4d39-8357-9579b2440e14",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3f837-6608-4e4b-ad18-ba208a798b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.losses import *\n",
    "from tensorflow.keras.layers import *\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "def predict_next_word(n):\n",
    "    \n",
    "    model = load_model('model_6.keras')\n",
    "    model.compile(loss = \"mean_squared_error\")\n",
    "    \n",
    "    #Enter The Input\n",
    "    x = input(\"Enter Sentence :\")\n",
    "        \n",
    "    y = x.strip()\n",
    "    x = x.strip()\n",
    "    x = x.split(\" \")\n",
    "    t = x[len(x)-5]+\" \"+ x[len(x)-4]+\" \"+x[len(x)-3]+\" \"+x[len(x)-2] + \" \" + x[len(x)-1]\n",
    "        \n",
    "    sample = {}\n",
    "    sample.update({'lines':[t]})\n",
    "    sample = pd.DataFrame(sample)\n",
    "        \n",
    "    Text = tokenizer_y.texts_to_sequences(sample['lines'].values)\n",
    "    Text = pad_sequences(Text,maxlen = 5)\n",
    "    dict = tokenizer_y.word_index\n",
    "    inv_map = {v: k for k, v in dict.items()}\n",
    "    s = model.predict(Text)\n",
    "    prediction = list(s[0])\n",
    "    \n",
    "    for i in range(n):\n",
    "        maxi = max(prediction)\n",
    "        index = prediction.index(maxi)\n",
    "        final_output = y + \" \" + inv_map[index]\n",
    "        print(final_output)\n",
    "        print(\"Probability :\",maxi)\n",
    "        prediction.remove(maxi)\n",
    "\n",
    "##Output Top 5 Best Predictions\n",
    "predict_next_word(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac6d9d-ee85-4e61-b372-d1d95b3c9f9c",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "#### pip install tensorflow==2.16.1\n",
    "#### pip install keras==3.3.3\n",
    "#### pip install pandas==2.1.4\n",
    "#### pip install scikit-learn\n",
    "#### pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d1d23c-52a6-4097-ad96-843e814a6b16",
   "metadata": {},
   "source": [
    "# Author"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b8b87-217b-4b73-9192-ac216cf5b4de",
   "metadata": {},
   "source": [
    "## Aryan Kaul"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
